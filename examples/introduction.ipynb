{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Intro to Developmental Interpretability\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timaeus-research/devinterp/blob/main/examples/introduction.ipynb)\n",
                "\n",
                "Developmental interpretability (\"devinterp\") studies how structure develops over the course of training. \n",
                "\n",
                "## Aim\n",
                "\n",
                "Our aim is to develop tools for detecting, understanding, and preventing dangerous transitions during training — to capabilities, values, behaviors. \n",
                "\n",
                "This library is where we'll put those tools. **It's very much a work in progress** (and we welcome contributions)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Set-up"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: transformers in /home/svwin/timaeus/.venv/lib/python3.8/site-packages (4.44.0)\n",
                        "Requirement already satisfied: torchvision==0.17 in /home/svwin/timaeus/.venv/lib/python3.8/site-packages (0.17.0)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "%pip install -U --no-deps transformers torchvision==0.17\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Methods\n",
                "\n",
                "## Local Learning Coefficients\n",
                "\n",
                "The first method we have have online is local learning coefficient estimation ([Lau et al. 2023](https://arxiv.org/abs/2308.12108)). \n",
                "\n",
                "For an in-depth explaination, see [this post](https://www.lesswrong.com/posts/6g8cAftfQufLmFDYT/you-re-counting-your-parameters-wrong). The short version is that: \n",
                "- The (local) learning coefficient $\\hat\\lambda$ is the \"correct\" measure of model complexity. Besides the loss, it's the most principled high-level way to compare models.\n",
                "- We can cheaply estimate the learning coefficient associated to a choice of weights $\\hat w^*$ by using the following formula:\n",
                "\n",
                "$$\n",
                "\\hat\\lambda(\\hat w^*)=\\frac{\\mathbb{E}_{w|\\hat w^*, \\gamma}^{\\beta^*}\\left[n L_n(w)\\right] - n L_n(\\hat w^*)}{\\log n}.\n",
                "$$\n",
                "\n",
                "where, $n$ is the number of (training) samples, $L_n(w)$ is the negative log likelihood (i.e., your objective function), and $\\mathbb{E}_w^{\\beta^*}\\left[n L_n(w)\\right]$ is a \"local average\" of $nL_n(w)$ over a neighborhood of $\\hat w^*$. The parameters $\\gamma$ and $\\beta^*$ control the local averaging.\n",
                "\n",
                "The difficulty is in performing the local average: we need to sample enough points and they need to be both diverse / spread out enough and close enough to the starting weight. To make sure they're spread out enough we use an optimizer like Stochastic Gradient Langevin Dynamics (SGLD), which is SGD plus Gaussian noise.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/svwin/timaeus/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
                        "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "USING TPU BACKEND\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch_xla.core.xla_model as xm\n",
                "from torch.nn import functional as F\n",
                "import torchvision\n",
                "from transformers import AutoModelForImageClassification\n",
                "\n",
                "from devinterp.slt.sampler import estimate_learning_coeff_with_summary\n",
                "from devinterp.optim import SGLD\n",
                "from devinterp.utils import Outputs, plot_trace\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "DEVICE  = xm.xla_device()\n",
                "\n",
                "\n",
                "\n",
                "def evaluate(model, data):\n",
                "    inputs, outputs = data\n",
                "\n",
                "    return F.cross_entropy(\n",
                "        model(inputs).logits, outputs\n",
                "    ), {\"logits\": model(inputs).logits}  # transformers doesn't output a vector. TODO FIX NON TPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[xla:0] Chain 0: 100%|██████████| 300/300 [00:32<00:00,  9.10it/s, grad_accum_steps=0]\n",
                        "[xla:0] Chain 1: 100%|██████████| 300/300 [00:15<00:00, 19.55it/s, grad_accum_steps=0]\n",
                        "[xla:0] Chain 2: 100%|██████████| 300/300 [00:15<00:00, 19.32it/s, grad_accum_steps=0]\n",
                        "[xla:0] Chain 3: 100%|██████████| 300/300 [00:16<00:00, 18.65it/s, grad_accum_steps=0]\n",
                        "[xla:0] Chain 4: 100%|██████████| 300/300 [00:16<00:00, 18.47it/s, grad_accum_steps=0]\n"
                    ]
                },
                {
                    "ename": "AttributeError",
                    "evalue": "'float' object has no attribute 'cpu'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(\n\u001b[1;32m      4\u001b[0m     root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     ),\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m learning_coeff_stats \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_learning_coeff_with_summary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSGLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# How many independent chains to run\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_draws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# How many samples to draw per chain\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_burnin_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# How many samples to discard at the beginning of each chain\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps_bw_draws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# How many steps to take between each sample\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43monline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m trace \u001b[38;5;241m=\u001b[39m learning_coeff_stats\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllc/trace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/devinterp/src/devinterp/slt/sampler.py:65\u001b[0m, in \u001b[0;36mestimate_learning_coeff_with_summary\u001b[0;34m(model, loader, callbacks, evaluate, sampling_method, optimizer_kwargs, num_draws, num_chains, num_burnin_steps, num_steps_bw_draws, init_loss, grad_accum_steps, cores, seed, device, verbose, optimize_over_per_model_param, online)\u001b[0m\n\u001b[1;32m     59\u001b[0m     llc_estimator \u001b[38;5;241m=\u001b[39m LLCEstimator(\n\u001b[1;32m     60\u001b[0m         num_chains, num_draws, optimizer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m], init_loss\u001b[38;5;241m=\u001b[39minit_loss, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     63\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [llc_estimator, \u001b[38;5;241m*\u001b[39mcallbacks]\n\u001b[0;32m---> 65\u001b[0m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_draws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_draws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_chains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_burnin_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_burnin_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps_bw_draws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps_bw_draws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize_over_per_model_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize_over_per_model_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m callbacks:\n",
                        "File \u001b[0;32m~/devinterp/src/devinterp/backends/tpu/slt/sampler.py:397\u001b[0m, in \u001b[0;36msample\u001b[0;34m(model, loader, callbacks, evaluate, sampling_method, optimizer_kwargs, scheduler_cls, scheduler_kwargs, num_draws, num_chains, num_burnin_steps, num_steps_bw_draws, grad_accum_steps, cores, seed, device, verbose, optimize_over_per_model_param, batch_size, init_noise, shuffle, use_alternate_batching)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m callbacks_list:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(callback, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinalize\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 397\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, \u001b[38;5;28mdict\u001b[39m):\n",
                        "File \u001b[0;32m~/devinterp/src/devinterp/backends/tpu/slt/llc.py:125\u001b[0m, in \u001b[0;36mOnlineLLCEstimator.finalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    122\u001b[0m cum_loss_avgs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcumsum(np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_draws \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    124\u001b[0m )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllcs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbeta \u001b[38;5;241m*\u001b[39m (cum_loss_avgs \u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mnumpy())\n",
                        "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'cpu'"
                    ]
                }
            ],
            "source": [
                "# Load a pretrained MNIST classifier\n",
                "model = AutoModelForImageClassification.from_pretrained(\"fxmarty/resnet-tiny-mnist\").to(DEVICE)\n",
                "data = torchvision.datasets.MNIST(\n",
                "    root=\"../data\",\n",
                "    download=True,\n",
                "    transform=torchvision.transforms.Compose(\n",
                "        [\n",
                "            torchvision.transforms.ToTensor(),\n",
                "        ]\n",
                "    ),\n",
                ")\n",
                "loader = torch.utils.data.DataLoader(data, batch_size=256, shuffle=True)\n",
                "\n",
                "learning_coeff_stats = estimate_learning_coeff_with_summary(\n",
                "    model,\n",
                "    loader=loader,\n",
                "    evaluate=evaluate,\n",
                "    sampling_method=SGLD,\n",
                "    optimizer_kwargs=dict(lr=4e-4, localization=100.0),\n",
                "    num_chains=3,  # How many independent chains to run\n",
                "    num_draws=100,  # How many samples to draw per chain\n",
                "    num_burnin_steps=0,  # How many samples to discard at the beginning of each chain\n",
                "    num_steps_bw_draws=1,  # How many steps to take between each sample\n",
                "    device=DEVICE,\n",
                "    online=True\n",
                ")\n",
                "\n",
                "trace = learning_coeff_stats.pop(\"llc/trace\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Diagnostics\n",
                "\n",
                "Below you'll see what's actually happening when you run `local_learning_coefficients`.\n",
                "\n",
                "We sample 10 different chains, with the same starting positions but different batch schedules and noise realizations at each step. For each of these chains, we take 200 steps using SGLD. We observe the loss at each of these points. At the end, we average the loss across chains, compare it to the initial loss, and apply a correction that depends on the dataset size to get the local learning coefficient. \n",
                "\n",
                "For a healthy chain, the LLC should increase rapidly at first and then level off."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_trace(\n",
                "    trace,\n",
                "    \"LLC\",\n",
                "    x_axis=\"Step\",\n",
                "    title=\"LLC Trace\",\n",
                "    plot_mean=False,\n",
                "    plot_std=False,\n",
                "    fig_size=(12, 9),\n",
                "    true_lc=None,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To get a better understanding of the sampling methods, check out:\n",
                "-  [`normal_crossing.ipynb`](../examples/normal_crossing.ipynb) showing both SGLD and SGNHT get the right answers in known simple cases.\n",
                "-  [`mnist.ipynb`](../examples/mnist.ipynb) showing how we can use LLC estimation to assess relative LLCs of MNIST models trained with different optimizers.\n",
                "- [`sgld_calibration.ipynb`](../examples/sgld_calibration.ipynb) shows how to gain confidence in using SGLD-based LLC estimation to a model with unknown LLC.\n",
                "- [`diagnostics.ipynb`](../examples/diagnostics.ipynb) shows how to use callbacks to diagnose if your sampling is going well.\n",
                "- [`epsilon_beta.ipynb`](../examples/epsilon_beta.ipynb) shows how to use use a callback to calibrate SGLD hyperparameters."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Project ideas\n",
                "\n",
                "What can you do with this? We wrote up [a bunch of ideas](https://devinterp.com/projects), most of which are focused on learning coefficient estimation, and we'd recommend starting there. There's a lot of low-hanging fruit. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Other Techniques\n",
                "\n",
                "There are plenty of other techniques that would fit naturally under the umbrella of \"developmental interpretability.\" If you're interested in looking beyond learning coefficient estimation, here are some ideas (we welcome PRs to introduce these methods to the library!):\n",
                "\n",
                "- **Dimensionality reduction techniques**: Run PCA / t-SNE / UMAP / etc. over the weights / [tokens](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#per-token-loss-intro) / activations over time. For larger models, you'll have to restrict your attention to a subset of the weights and activations for this to be tractable\n",
                "- **Progress measures**. If you have an understanding of some structure at the end of training, you can roll that understanding backwards to track how that structure develops over time. \n",
                "- **Probes**. Similarly, you can train a linear probe from activations onto features, then roll that probe back to previous checkpoints to measure how those features are learned. \n",
                "- **Gradients**. Just look at the gradients! \n",
                "- **Evals**. You can measure performance on a targeted benchmarks to track when the model learns the associated capabilities. \n",
                "- **Covariance estimators**. That's a secret for now. More coming soon!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Systems\n",
                "\n",
                "## Toy Models of Superposition\n",
                "\n",
                "See [`tms.ipynb`](../examples/tms.ipynb) for a demo of how to use the library on toy models of superposition.\n",
                "\n",
                "## Deep Linear Networks\n",
                "\n",
                "See [`dlns.ipynb`](../examples/dlns.ipynb) for a demo of how to use the library on deep linear networks.\n",
                "\n",
                "## MNIST\n",
                "\n",
                "See [`mnist.ipynb`](../examples/mnist.ipynb) for a demo of how to use the library on MNIST.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
